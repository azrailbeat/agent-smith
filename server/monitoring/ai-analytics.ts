/**
 * Сервис для ИИ-анализа данных мониторинга LLM моделей
 */

import OpenAI from 'openai';
import { logActivity } from '../activity-logger';
import { ActivityType } from '../activity-logger';
import { AIAnalysisResponse, ModelUsage, ServiceStatus } from '../types/monitoring';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

/**
 * Обработка запроса на анализ данных LLM с помощью ИИ
 */
export async function processAIAnalysisRequest(
  llmUsage: ModelUsage[],
  llmStatus: ServiceStatus[],
  analysisType: string,
  userId?: number
): Promise<AIAnalysisResponse> {
  try {
    // Проверяем наличие API ключа OpenAI
    if (!process.env.OPENAI_API_KEY) {
      console.warn('OPENAI_API_KEY не установлен - невозможно выполнить ИИ-анализ');
      return {
        success: false,
        analysisType,
        content: 'Для выполнения ИИ-анализа требуется настроенный ключ API OpenAI. Пожалуйста, добавьте ключ в настройках системы.',
        metadata: {
          error: 'OPENAI_API_KEY не найден в переменных окружения',
          timestamp: new Date().toISOString()
        }
      };
    }

    // Подготавливаем данные для анализа
    const inputData = {
      usage: llmUsage,
      status: llmStatus,
      timestamp: new Date().toISOString(),
    };

    // Создаем системный промпт в зависимости от типа анализа
    let systemPrompt = 'Вы - эксперт по анализу производительности и оптимизации LLM моделей. ';
    let userPrompt = '';

    switch (analysisType) {
      case 'optimization':
        systemPrompt += 'Ваша задача - проанализировать данные мониторинга LLM моделей и предоставить рекомендации по оптимизации их использования с точки зрения стоимости, эффективности и производительности.';
        userPrompt = `Пожалуйста, проанализируйте следующие данные мониторинга LLM моделей и предоставьте конкретные рекомендации по оптимизации их использования, обращая особое внимание на стоимость, эффективность и производительность:

Данные мониторинга:
${JSON.stringify(inputData, null, 2)}

Сфокусируйтесь на следующих аспектах:
1. Оптимизация стоимости использования LLM моделей
2. Оптимизация производительности (время ответа)
3. Эффективное использование токенов
4. Распределение ресурсов между сервисами
5. Конкретные рекомендации для каждой модели

Предоставьте ответ в формате Markdown с заголовками, списками и выделениями важной информации.`;
        break;

      case 'trends':
        systemPrompt += 'Ваша задача - проанализировать тренды в использовании LLM моделей и предоставить прогнозы будущего использования и рекомендации.';
        userPrompt = `Пожалуйста, проанализируйте следующие данные мониторинга LLM моделей, выявите тренды в их использовании и предоставьте прогнозы и рекомендации:

Данные мониторинга:
${JSON.stringify(inputData, null, 2)}

Сфокусируйтесь на следующих аспектах:
1. Тенденции использования разных моделей (рост/снижение)
2. Распределение запросов между моделями
3. Прогноз затрат в будущем при сохранении текущих трендов
4. Рекомендации по масштабированию на основе трендов
5. Сезонные или временные паттерны, если они заметны

Предоставьте ответ в формате Markdown с заголовками, списками и выделениями важной информации.`;
        break;

      case 'alerts':
        systemPrompt += 'Ваша задача - выявить проблемные зоны и потенциальные узкие места в работе LLM моделей и сервисов на основе данных мониторинга.';
        userPrompt = `Пожалуйста, проанализируйте следующие данные мониторинга LLM моделей и выявите проблемные зоны, потенциальные узкие места и критические ситуации, требующие внимания:

Данные мониторинга:
${JSON.stringify(inputData, null, 2)}

Сфокусируйтесь на следующих аспектах:
1. Проблемы в работе сервисов (деградация, недоступность)
2. Потенциальные проблемы с затратами (высокое потребление ресурсов)
3. Проблемы производительности (высокое время ответа)
4. Конкретные рекомендации по устранению выявленных проблем
5. Приоритизация проблем по их критичности

Используйте символы ✅ для обозначения нормальной работы, ⚠️ для предупреждений и ❌ для критических проблем.
Предоставьте ответ в формате Markdown с заголовками, списками и выделениями важной информации.`;
        break;

      case 'comprehensive':
        systemPrompt += 'Ваша задача - провести комплексный анализ данных мониторинга LLM моделей, включающий оптимизацию, тренды и выявление проблемных зон.';
        userPrompt = `Пожалуйста, проведите комплексный анализ следующих данных мониторинга LLM моделей:

Данные мониторинга:
${JSON.stringify(inputData, null, 2)}

Предоставьте полный аналитический отчет, включающий:

1. ОБЩИЙ ОБЗОР
   - Ключевые метрики и их интерпретация
   - Общая оценка состояния системы LLM

2. ОПТИМИЗАЦИЯ
   - Рекомендации по оптимизации стоимости
   - Рекомендации по оптимизации производительности
   - Рекомендации по эффективному использованию токенов

3. ТРЕНДЫ И ПРОГНОЗЫ
   - Тенденции использования разных моделей
   - Прогноз будущих затрат и использования
   - Рекомендации по масштабированию

4. ПРОБЛЕМНЫЕ ЗОНЫ И ПРЕДУПРЕЖДЕНИЯ
   - Выявленные проблемы в работе сервисов
   - Потенциальные узкие места системы
   - Рекомендации по устранению проблем

5. ПРИОРИТЕТНЫЕ ДЕЙСТВИЯ
   - Список действий, требующих немедленного внимания
   - Среднесрочные рекомендации
   - Долгосрочные стратегические рекомендации

Используйте ✅ для обозначения нормальной работы, ⚠️ для предупреждений и ❌ для критических проблем.
Предоставьте ответ в формате Markdown с заголовками, списками и выделениями важной информации.`;
        break;

      default:
        systemPrompt += 'Ваша задача - проанализировать данные мониторинга LLM моделей и предоставить общие рекомендации по их использованию.';
        userPrompt = `Пожалуйста, проанализируйте следующие данные мониторинга LLM моделей и предоставьте общие рекомендации:

Данные мониторинга:
${JSON.stringify(inputData, null, 2)}

Предоставьте ответ в формате Markdown с заголовками, списками и выделениями важной информации.`;
    }

    // Вызываем OpenAI для анализа данных
    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      max_tokens: 2000,
      temperature: 0.3,
    });

    // Получаем результат анализа
    const analysisContent = response.choices[0].message.content || 'Не удалось получить результат анализа.';
    const tokensUsed = response.usage?.total_tokens || 0;

    // Логируем использование ИИ-анализа в системе
    await logActivity({
      action: 'AI_LLM_ANALYSIS',
      entityType: 'llm_monitoring',
      entityId: 0, // ID записи мониторинга (если необходимо)
      userId,
      details: `ИИ-анализ типа "${analysisType}" выполнен`,
      metadata: {
        analysisType,
        tokensUsed,
        timestamp: new Date().toISOString()
      }
    });

    return {
      success: true,
      analysisType,
      content: analysisContent,
      metadata: {
        tokensUsed,
        modelUsed: 'gpt-4',
        generationMethod: 'openai_chat_completions',
        timestamp: new Date().toISOString()
      }
    };
  } catch (error) {
    console.error('Ошибка при выполнении ИИ-анализа:', error);

    // Логируем ошибку
    await logActivity({
      action: 'ERROR',
      entityType: 'llm_monitoring',
      entityId: 0,
      userId,
      details: `Ошибка при выполнении ИИ-анализа типа "${analysisType}"`,
      metadata: {
        error: error instanceof Error ? error.message : 'Неизвестная ошибка',
        timestamp: new Date().toISOString()
      }
    });

    return {
      success: false,
      analysisType,
      content: 'Произошла ошибка при выполнении ИИ-анализа данных. Пожалуйста, проверьте логи сервера для получения дополнительной информации.',
      metadata: {
        error: error instanceof Error ? error.message : 'Неизвестная ошибка',
        timestamp: new Date().toISOString()
      }
    };
  }
}